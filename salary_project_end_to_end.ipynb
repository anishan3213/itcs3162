{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05368b9c",
   "metadata": {},
   "source": [
    "\n",
    "# Salary Prediction — Simple Linear Regression (Kaggle)\n",
    "\n",
    "**Dataset used:** *Years of experience and Salary dataset* (by rohankayan) — contains **`Salary_Data.csv`**.  \n",
    "Kaggle link: https://www.kaggle.com/datasets/rohankayan/years-of-experience-and-salary-dataset\n",
    "\n",
    "This notebook is built to **read exactly from `Salary_Data.csv`** placed in the same folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518ce515",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd6678",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33669329",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Introduction — Problem & Dataset (Rubric: Intro 5 pts)\n",
    "We predict **Salary** from **Years of Experience** using simple regression.  \n",
    "The Kaggle dataset provides a compact CSV (`Salary_Data.csv`) widely used for regression demos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c572a48",
   "metadata": {},
   "source": [
    "## 2) Load the Dataset (reads `Salary_Data.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e599125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CSV_PATH = 'Salary_Data.csv'  # exact filename from the Kaggle dataset\n",
    "\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    raise FileNotFoundError(\"Could not find Salary_Data.csv. Download from Kaggle and place it next to this notebook.\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head())\n",
    "display(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a07b6ac",
   "metadata": {},
   "source": [
    "\n",
    "## 3) What is Regression? (Rubric: 10 pts + bonus math)\n",
    "**Regression** predicts a continuous target variable (here, salary).  \n",
    "**Simple Linear Regression** models a linear relationship:\n",
    "\\(\n",
    "y = \\beta_0 + \\beta_1 x + \\varepsilon\n",
    "\\)\n",
    "We estimate parameters by minimizing squared error. Closed-form solution:\n",
    "\\(\n",
    "\\hat{\\\\beta} = (X^TX)^{-1}X^Ty\n",
    "\\)\n",
    "**RMSE** is used to measure error:\n",
    "\\(\n",
    "\\\\text{RMSE} = \\\\sqrt{\\\\frac{1}{n}\\\\sum (y_i - \\\\hat{y}_i)^2}\n",
    "\\)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d765cc",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Experiment 1 — Data Understanding (Rubric: 5 pts)\n",
    "We examine distributions and relationships to form initial hypotheses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc634b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scatter plot: YearsExperience vs Salary\n",
    "plt.figure()\n",
    "plt.scatter(df['YearsExperience'], df['Salary'])\n",
    "plt.title('YearsExperience vs Salary')\n",
    "plt.xlabel('YearsExperience')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()\n",
    "\n",
    "# Check simple correlation\n",
    "corr = df.corr(numeric_only=True)\n",
    "print(\"Correlation matrix:\")\n",
    "display(corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8352aae",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Experiment 1 — Pre-processing (Rubric: 5 pts)\n",
    "This dataset has two numeric columns and no missing values typically. We'll still set up a robust pipeline:\n",
    "- Train/validation split (80/20)\n",
    "- Standardization for numerical stability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b26e937",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df[['YearsExperience']].values\n",
    "y = df['Salary'].values\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "print(\"Train size:\", X_train.shape[0], \" | Valid size:\", X_valid.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f864dcb",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Experiment 1 — Modeling (Linear Regression) (Rubric: 5 pts)\n",
    "Baseline **LinearRegression** with standardization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2689e31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe_lr = Pipeline([('scaler', StandardScaler()), ('model', LinearRegression())])\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "\n",
    "pred_lr = pipe_lr.predict(X_valid)\n",
    "rmse_lr = rmse(y_valid, pred_lr)\n",
    "r2_lr = r2_score(y_valid, pred_lr)\n",
    "print(\"EXP 1 — Linear | RMSE:\", rmse_lr, \"| R^2:\", r2_lr)\n",
    "\n",
    "# 5-fold CV for stability (on full data)\n",
    "cv_scores = cross_val_score(pipe_lr, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "print(\"EXP 1 — 5-fold CV RMSE (mean ± std):\", -cv_scores.mean(), \"±\", cv_scores.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d0d67f",
   "metadata": {},
   "source": [
    "### Residual Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94259d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resid = y_valid - pred_lr\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(pred_lr, resid, s=10)\n",
    "plt.axhline(0)\n",
    "plt.title('Residuals vs Predictions (Linear)')\n",
    "plt.xlabel('Predicted Salary'); plt.ylabel('Residual')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(resid, bins=10)\n",
    "plt.title('Residuals Histogram (Linear)')\n",
    "plt.xlabel('Residual'); plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8278f8d",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Experiment 2 — Regularized Linear Models (Ridge & Lasso) (Rubric: 15 pts)\n",
    "With such a small dataset, regularization may help generalization slightly. We tune alpha by simple grid search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92849931",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alphas = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "best_rmse_ridge, best_alpha_ridge = float('inf'), None\n",
    "for a in alphas:\n",
    "    ridge = Pipeline([('scaler', StandardScaler()), ('model', Ridge(alpha=a, random_state=RANDOM_STATE))])\n",
    "    ridge.fit(X_train, y_train)\n",
    "    pred = ridge.predict(X_valid)\n",
    "    score = rmse(y_valid, pred)\n",
    "    if score < best_rmse_ridge:\n",
    "        best_rmse_ridge, best_alpha_ridge = score, a\n",
    "\n",
    "best_rmse_lasso, best_alpha_lasso = float('inf'), None\n",
    "for a in alphas:\n",
    "    lasso = Pipeline([('scaler', StandardScaler()), ('model', Lasso(alpha=a, random_state=RANDOM_STATE, max_iter=10000))])\n",
    "    lasso.fit(X_train, y_train)\n",
    "    pred = lasso.predict(X_valid)\n",
    "    score = rmse(y_valid, pred)\n",
    "    if score < best_rmse_lasso:\n",
    "        best_rmse_lasso, best_alpha_lasso = score, a\n",
    "\n",
    "print(f\"EXP 2 — Ridge best alpha={best_alpha_ridge} | RMSE={best_rmse_ridge}\")\n",
    "print(f\"EXP 2 — Lasso best alpha={best_alpha_lasso} | RMSE={best_rmse_lasso}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3bfbdd",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Experiment 3 — Polynomial Regression (degree=2,3) (Rubric: 15 pts)\n",
    "We test whether a mild nonlinearity improves fit without overfitting (compare degrees). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c5e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "degrees = [1,2,3]\n",
    "results_poly = []\n",
    "\n",
    "for d in degrees:\n",
    "    poly = Pipeline([('poly', PolynomialFeatures(degree=d, include_bias=False)),\n",
    "                     ('scaler', StandardScaler()),\n",
    "                     ('model', LinearRegression())])\n",
    "    poly.fit(X_train, y_train)\n",
    "    pred = poly.predict(X_valid)\n",
    "    results_poly.append((d, rmse(y_valid, pred), r2_score(y_valid, pred)))\n",
    "\n",
    "results_poly_df = pd.DataFrame(results_poly, columns=['Degree','Valid_RMSE','Valid_R2']).sort_values('Valid_RMSE')\n",
    "display(results_poly_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5766a1",
   "metadata": {},
   "source": [
    "## 9) Compare All Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2698ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary = pd.DataFrame({\n",
    "    'Experiment': ['Linear (Std)', 'Ridge (best)', 'Lasso (best)', 'Poly best (by RMSE)'],\n",
    "    'Valid_RMSE': [rmse_lr, best_rmse_ridge, best_rmse_lasso, results_poly_df.iloc[0]['Valid_RMSE']],\n",
    "    'Valid_R2':   [r2_lr, np.nan, np.nan, results_poly_df.iloc[0]['Valid_R2']]\n",
    "})\n",
    "display(summary.sort_values('Valid_RMSE'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bf2b2a",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Impact (Rubric: 5 pts)\n",
    "Salary models risk encoding inequities if trained with biased factors (e.g., education, location, demographics). \n",
    "Even with a neutral feature like YearsExperience, deploying salary predictors may influence negotiation dynamics and fairness. \n",
    "Mitigations: transparency about uncertainty, avoid sensitive attributes, and use human oversight.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab32398",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Conclusion (Rubric: 10 pts)\n",
    "- Linear regression provides a strong baseline with clear interpretability.\n",
    "- On this tiny dataset, **regularization** may modestly help; **polynomial** terms can overfit—use CV to verify.\n",
    "- With more data, we’d expand features (role, industry, location) and audit subgroup errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b714cb",
   "metadata": {},
   "source": [
    "\n",
    "## 12) References (Rubric: 5 pts)\n",
    "- Kaggle dataset: *Years of experience and Salary dataset* (rohankayan).\n",
    "- scikit-learn documentation for LinearRegression, Ridge, Lasso, PolynomialFeatures.\n",
    "- Course notes on regression and RMSE.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
